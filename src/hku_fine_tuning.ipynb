{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"hku_fine_tuning.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"X1eIBWYV7HAI"},"source":["### Set up environment"]},{"cell_type":"code","metadata":{"id":"K_WGjVgS7vdR"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!ln -s /content/drive/My\\ Drive/Colab\\ Notebooks/hku-oph/* /content/\n","%cd /content/src\n","\n","!pip install scikit-plot\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"start_time":"2020-05-16T10:51:22.566Z"},"id":"5qIokD-57HAJ"},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","        \n","# ML libraries required\n","from fastai import *\n","from fastai.vision import *\n","from fastai.callbacks import *\n","from fastai.metrics import KappaScore # solution evaluated with qudratic kappa\n","from fastai.tabular import * # for ensemble model training\n","import torch\n","# efficientnet is not integrated into fastai yet\n","\n","\n","# Other libraries required\n","import matplotlib.pyplot as plt\n","from models.efficientnet_pytorch import EfficientNet\n","\n","# garbage collector\n","import gc\n","\n","import random\n","from datetime import datetime\n","\n","def seed_everything(seed_value, use_cuda=True):\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","    np.random.seed(seed_value) # cpu vars\n","    torch.manual_seed(seed_value) # cpu  vars\n","    random.seed(seed_value) # Python\n","    if use_cuda: \n","        torch.cuda.manual_seed(seed_value)\n","        torch.cuda.manual_seed_all(seed_value) # gpu vars\n","        torch.backends.cudnn.deterministic = True  #needed\n","        torch.backends.cudnn.benchmark = False\n","    \n","seed_everything(42, True)\n","\n","from sklearn.model_selection import StratifiedKFold"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1d46KzCX7HAN"},"source":["### Define model details"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-05-13T17:46:27.239529Z","start_time":"2020-05-13T17:46:27.236883Z"},"id":"UxIWbY--7HAN"},"source":["import json\n","models_config = {}\n","only_train_model = None\n","with open('fine_tune_config.json') as f:\n","    models_config = json.load(f)\n","    meta_config = models_config[\"meta\"]\n","    ensemble_config = models_config[\"ensemble\"]\n","    models_config = models_config[\"models\"]\n","    \n","if \"only_train_model\" in meta_config:\n","    only_train_model = meta_config[\"only_train_model\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-PviswI7HAP"},"source":["### Import preprocessing modules"]},{"cell_type":"code","metadata":{"id":"oW6lGw8U7HAQ"},"source":["%run preprocessing.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xS9ZDF9k7HAT"},"source":["### Load data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"start_time":"2020-05-16T11:04:16.483Z"},"id":"MVG-7AN37HAU"},"source":["class PreProcessCommonWrapper(object):\n","    def __init__(self, image_dim):\n","        self.image_dim = image_dim\n","        self.__name__ = \"PreProcessCommonWrapper\"\n","        self.__annotations__ = {}\n","    def __call__(self, t): # the function formerly known as \"bar\"\n","        return contrast_and_crop(t, self.image_dim)\n","\n","def get_df():\n","    base_dir = os.path.join('../', 'datasets/hkuretinopathydataset/')\n","    train_dir = os.path.join('../', 'datasets/hkuretinopathydataset/')\n","    \n","    df = pd.read_csv(os.path.join(base_dir, 'images_2_fields.csv'))\n","    \n","    # remap the diagnosis axis\n","    df.loc[df.diagnosis == 3, \"diagnosis\"] = 4\n","    df.loc[df.diagnosis == 2.5, \"diagnosis\"] = 3\n","    return df, base_dir, train_dir\n","    \n","def load_data(model_config, train_index, val_index):\n","    current_model_config = model_config\n","    print(current_model_config)\n","    df, base_dir, train_dir = get_df()\n","    df['path'] = df['path'].map(lambda x: os.path.join(train_dir, '/'.join(x.split('/')[-2:])))\n","    # df = df.sample(frac=1, random_state=42)\n","    src = ImageList.from_df(df=df, path = './', cols='path') \\\n","                   .split_by_idxs(train_index, val_index) \\\n","                   .label_from_df(cols='diagnosis', label_cls=FloatList)  # although labels are in integer form, they are intepreted as Float for training purposes\n","                   \n","            \n","    transformations = get_transforms(do_flip=True,flip_vert=True,max_rotate=360,max_warp=0,max_zoom=1.3,max_lighting=0.1,p_lighting=0.5)\n","    \n","    # custom pre-processing (contrast and crop)\n","    pre_process_common_wrapper = PreProcessCommonWrapper(model_config[\"image_dim\"])\n","    pre_process_ccs = [TfmPixel(pre_process_common_wrapper)()]\n","    advprop = model_config[\"advprop\"]\n","    if advprop:\n","        pre_process_ccs.append(TfmPixel(advprop_normalise)())\n","    # apply transformations to training set, but apply the pre_process to train and valid set\n","    tfms = [transformations[0] + pre_process_ccs, transformations[1] + pre_process_ccs]\n","    \n","    # transform data sets\n","    data = src.transform(tfms, size=model_config[\"image_dim\"], resize_method=ResizeMethod.CROP,padding_mode='zeros',) \\\n","              .databunch(bs=model_config[\"batch_size\"], num_workers=1) \\\n","              .normalize(imagenet_stats if not advprop else None) # default normalise with imagenet stats, prebuilt into fast.ai library    \n","    # visualise this batch\n","    # data.show_batch(rows=3, figsize=(10,10), ds_type=DatasetType.Valid)\n","\n","    print(\"loaded data\")\n","    return (df, data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ytB-3NiO7HAW"},"source":["### Helper functions in loading models"]},{"cell_type":"code","metadata":{"id":"J9c-coSe7HAX"},"source":["def getModel(model_name, data, model_dir=None, advprop=False, **kwargs):\n","    from os.path import abspath\n","    if model_dir is not None:\n","        model_dir = abspath(model_dir)\n","    model = EfficientNet.from_pretrained(model_name, advprop=advprop)\n","    model._fc = nn.Linear(model._fc.in_features,data.c) # .c returns number of output cells, ._fc returns the module\n","    return model\n","\n","def get_learner(model_name, data, model_dir=\"models/\"):\n","    return Learner(data, getModel(model_name, data, model_dir=model_dir), metrics = [quadratic_kappa]) \\\n","           .mixup() \\\n","           .to_fp16() \n","\n","# quadratic kappa score\n","from sklearn.metrics import cohen_kappa_score\n","def quadratic_kappa(y_hat, y):\n","    y_hat = y_hat.cpu()\n","    y = y.cpu()\n","    return torch.tensor(cohen_kappa_score(torch.round(y_hat), y, weights='quadratic'),device='cuda:0')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SiJQLTYRWV9u"},"source":["# lets visualise what we have got\n","import warnings \n","warnings.filterwarnings(\"ignore\")\n","# df, data = load_data(models_config[\"effnet-b3\"], range(0,800), range(801,1000))\n","# data.show_batch(rows=3, figsize=(10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVZrT8pR7HAZ"},"source":["### Train main models routine"]},{"cell_type":"code","metadata":{"id":"seSX0UKF7HAa"},"source":["def train_net(fold_n, train_index, val_index):\n","    valid_predictions = {}\n","    valid_labels = []\n","    ensemble_predictions = {}\n","    ensemble_labels = []\n","    for config_name in models_config:\n","        print(f\"---- TRAINING STARTING FOR {config_name} ----\")\n","        config = models_config[config_name]\n","        df, data = load_data(config, train_index, val_index)\n","        learner = get_learner(config[\"pretrained_name\"], data, model_dir=config[\"pretrained_path\"])\n","        # check if a previous training on this fold has been done\n","        path = config[\"pretrained_path\"]\n","        onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","        using_old_fold = False\n","\n","        for fn in onlyfiles:\n","            temp = fn.replace('.pth', '').split('_')\n","            if temp[0] == config_name and temp[1] == \"fold\" and temp[2] == str(fold_n):\n","                print(f\"-- LOADED PREVIOUS TRAINED FOLD #{fold_n} FOR {config_name} --\")\n","                learner.load(f'{config_name}_fold_{fold_n}')  # load trained weights\n","                using_old_fold = True\n","                break\n","\n","        if not using_old_fold:\n","          print(f\"-- LOADED PREVIOUS APTOS PRETRAINED FOR {config_name} --\")\n","          learner.load(f'{config_name}_best')  # load trained weights\n","\n","        lr = config[\"lr\"]\n","        # learner.lr_find()\n","        # learner.recorder.plot() \n","        # break\n","        \n","        # training starts here \n","        if not using_old_fold:  # only retrain if fold not trained before\n","            learner.fit_one_cycle(\n","                config[\"epoch_n\"], \n","                lr,\n","                callbacks=[SaveModelCallback(learner, every='improvement', monitor='valid_loss', name=f'{config_name}_fold_{fold_n}')]\n","            )\n","        \n","        # perform prediction on \n","        # ensemble_predictions[config_name], ensemble_labels = learner.get_preds(DatasetType.Train) # Using train set to train the ensemble --> when we have more data we should train it with valid data\n","        # ensemble_predictions[config_name] = ensemble_predictions[config_name].flatten().tolist()\n","        ensemble_predictions = None\n","        ensemble_labels = None\n","        valid_predictions[config_name], valid_labels = learner.get_preds(DatasetType.Valid)\n","        valid_predictions[config_name] = valid_predictions[config_name].flatten().tolist()\n","\n","        learner.show_results()\n","\n","        # vp = pd.DataFrame.from_dict({\"value\": valid_predictions[config_name]})\n","        # vp['ensemble'] = np.mean(vp, axis=1)\n","        # vp[\"diagnosis\"] = valid_labels\n","        # get_and_show_ROC(vp, fold_n)\n","        del learner\n","        gc.collect()\n","    return (ensemble_predictions, ensemble_labels, valid_predictions, valid_labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJkX7Qt47HAc"},"source":["### Ensemble layer"]},{"cell_type":"code","metadata":{"id":"XNte66go7HAd"},"source":["def train_ensemble_average(fold_n, ensemble_predictions, ensemble_labels):\n","    ensemble_predictions[\"diagnosis\"] = ensemble_labels\n","    df = pd.DataFrame(ensemble_predictions)\n","    procs = [Normalize]\n","    model_names = models_config.keys()\n","    ensemble_bunch = TabularList.from_df(df=df, cont_names=model_names, procs=procs) \\\n","                                .split_by_rand_pct() \\\n","                                .label_from_df(cols='diagnosis') \\\n","                                .databunch()\n","    \n","    learner = tabular_learner(ensemble_bunch, layers=[100, 50], ps=[0.5,0.2], metrics=[quadratic_kappa]) \n","\n","    lr = ensemble_config[\"lr\"]\n","    epoch_n =ensemble_config[\"epoch_n\"]\n","    learner.fit_one_cycle(\n","        epoch_n, \n","        lr, \n","        callbacks=[SaveModelCallback(learner, every='improvement', monitor='valid_loss', name=f'ensemble_{fold_n}')]\n","    )\n","\n","def simple_avg_ensemble(valid_predictions, valid_labels):\n","    valid_predictions = pd.DataFrame.from_dict(valid_predictions)\n","    valid_predictions['ensemble'] = np.mean(valid_predictions, axis=1)\n","    valid_predictions[\"diagnosis\"] = valid_labels\n","    return valid_predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"8x35k6Mo7HAf"},"source":["### ROC curve, sensitivity and specificity"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-05-16T13:35:08.333913Z","start_time":"2020-05-16T13:35:07.464977Z"},"hidden":true,"id":"H2jY5VTH7HAf"},"source":["%matplotlib inline\n","from sklearn.metrics import roc_curve, auc\n","from itertools import cycle\n","from scipy import interpolate\n","import matplotlib.pyplot as plt\n","import scikitplot as skplt\n","\n","def compress_predictions(df):\n","    # map predictions to 0 to 1, preserving ratio in between\n","    df.loc[df.ensemble < 0, \"ensemble\"] = 0\n","    df.loc[df.ensemble > 4, \"ensemble\"] = 4\n","    df = df.div(4)\n","    return df\n","\n","def compress_labels(df):\n","    # map labels to 0 or 1, 1 denoting requiring referral\n","    df.loc[df.diagnosis < 2, \"referral\"] = 0\n","    df.loc[df.diagnosis >= 2, \"referral\"] = 1\n","    df.referral = df.referral.astype(int)\n","    return df\n","\n","def get_and_show_ROC(df, fold_n):\n","    predictions = compress_predictions(df).ensemble\n","    labels = compress_labels(df).referral\n","    fpr, tpr, threshold = roc_curve(labels, predictions)\n","    auc_scaler = auc(fpr, tpr)\n","    print(f\"AUC: {auc_scaler}\")\n","    # TPR = sensitivity, FPR = 1 - specificity\n","    cutoff_df = pd.DataFrame({\n","        \"sensitivity\": tpr,\n","        \"specificity\": 1 - fpr,\n","        \"threshold\": threshold,\n","        \"auc\": auc_scaler\n","    })\n","    skplt.metrics.plot_roc(labels, pd.DataFrame({\"no\": 1 - predictions, \"yes\": predictions}))\n","    plt.savefig(f'roc_fold_{fold_n}.png')\n","    plt.show()\n","    return (cutoff_df, auc_scaler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2Y9k2UmiTsU"},"source":["### 2-field combined inference"]},{"cell_type":"code","metadata":{"id":"DGIthHd5njY6"},"source":["def integrate_2_fields(valid_predictions, valid_labels):\n","    valid_predictions = pd.DataFrame.from_dict(valid_predictions)\n","    valid_predictions['ensemble'] = np.mean(valid_predictions, axis=1)\n","    valid_predictions[\"diagnosis\"] = valid_labels\n","\n","    splits = np.array_split(valid_predictions, 2)\n","    splits[0][\"second_field\"] = list(splits[1][\"ensemble\"])\n","    results = splits[0].rename(columns={\"ensemble\": \"first_field\"})\n","    results[\"diff\"] = abs(results[\"first_field\"] - results[\"second_field\"])\n","\n","    # take simple average\n","    results['ensemble'] = (results['first_field'] + results['second_field']) / 2\n","\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bDXxAlJ87HAi"},"source":["### Entrypoint"]},{"cell_type":"code","metadata":{"id":"CBLEf1XW7HAi"},"source":["is_integrate_2_fields = True\n","\n","if __name__ == '__main__':\n","    # get folds\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","    df = get_df()[0]\n","    skf = StratifiedKFold(n_splits=meta_config[\"n_splits\"])\n","    i = 0\n","    for train_index, val_index in skf.split(df.index, df['diagnosis']):\n","        # skip to some folds\n","        # if i != 4:\n","        #   i += 1\n","        #   continue\n","        print(f'------ FOLD {i} ------')\n","        print(\"-- TRAINING INDICES --\")\n","        print(train_index)\n","        print(\"-- VALIDATION INDICES --\")\n","        print(val_index)\n","        ensemble_predictions, ensemble_labels, valid_predictions, valid_labels = train_net(i, train_index, val_index)\n","        # take a simple average for now\n","        if is_integrate_2_fields:\n","            valid_predictions_labels = integrate_2_fields(valid_predictions, valid_labels)\n","            valid_predictions_labels.to_csv(f'models/predictions_fold_{i}')\n","        else:\n","            valid_predictions_labels = simple_avg_ensemble(valid_predictions, valid_labels)\n","        cutoff_df, auc_scaler = get_and_show_ROC(valid_predictions_labels, i)\n","        cutoff_df.to_csv(f'models/cutoffs_fold_{i}')\n","        # train_ensemble_average(i, ensemble_predictions, ensemble_labels)\n","        # evaluate the senstivity and specificity of this fold\n","        i += 1"],"execution_count":null,"outputs":[]}]}
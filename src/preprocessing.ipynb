{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7-final"},"orig_nbformat":2,"kernelspec":{"name":"python36864bitvenvvenv21d10001fb6846a5a210f30db947f096","display_name":"Python 3.6.8 64-bit ('venv': venv)"},"colab":{"name":"preprocessing.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GG6gsrIdBG0f","colab_type":"text"},"source":["### Preprocessing for the models"]},{"cell_type":"code","metadata":{"id":"HVr5GIi0BG0q","colab_type":"code","colab":{},"outputId":"ef2c2364-a385-433f-b592-97e50f1a2875"},"source":["import cv2\n","import numpy as np\n","from fastai.vision import pil2tensor, Image\n","\n","def info_image(im):\n","    # Compute the center (cx, cy) and radius of the eye\n","    cy = im.shape[0]//2\n","    midline = im[cy,:]\n","    midline = np.where(midline>midline.mean()/3)[0]\n","    if len(midline)>im.shape[1]//2:\n","        x_start, x_end = np.min(midline), np.max(midline)\n","    else: # This actually rarely happens p~1/10000\n","        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10\n","    cx = (x_start + x_end)/2\n","    r = (x_end - x_start)/2\n","    return cx, cy, r\n","\n","def resize_image(im, image_dim=None, augmentation=False):\n","    # Crops, resizes and potentially augments the image to image_dim\n","    cx, cy, r = info_image(im)\n","    scaling = image_dim/(2*r)\n","    rotation = 0\n","    if augmentation:\n","        scaling *= 1 + 0.3 * (np.random.rand()-0.5)\n","        rotation = 360 * np.random.rand()\n","    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)\n","    M[0,2] -= cx - image_dim/2\n","    M[1,2] -= cy - image_dim/2\n","    # return im # disbale resizing\n","    return cv2.warpAffine(im,M,(image_dim,image_dim)) # This is the most important line\n","\n","def crop_image_from_gray(img,tol=7):\n","    \"\"\"\n","    Crop out black borders\n","    https://www.kaggle.com/ratthachat/aptos-updated-preprocessing-ben-s-cropping\n","    \"\"\"  \n","    if img.ndim == 2: # gray scale\n","        mask = img>tol  # create an array (mask) of pixels higher than tolerance\n","        return img[np.ix_(mask.any(1),mask.any(0))]\n","    elif img.ndim == 3: # color image\n","        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # convert to gray\n","        mask = gray_img>tol\n","        \n","        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n","        if (check_shape == 0): # image is too dark so that we crop out everything,\n","            return img # return original image\n","        else: \n","            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n","            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n","            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n","    #         print(img1.shape,img2.shape,img3.shape)\n","            img = np.stack([img1,img2,img3],axis=-1)\n","    #         print(img.shape)\n","        return img\n","\n","PARAM = 96\n","def Radius_Reduction(img,PARAM):\n","    h,w,c=img.shape\n","    Frame=np.zeros((h,w,c),dtype=np.uint8)\n","    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)\n","    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)\n","    img1 =cv2.bitwise_and(img,img,mask=Frame1)\n","    return img\n","    \n","def contrast_and_crop(torch_tensor, image_dim=None, path=None, sigmaX=10, median_blur=True):\n","    if path is None:\n","        np_image = image2np(torch_tensor) * 255 # convert tensor image to numpy array\n","        np_image = np_image.astype(np.uint8)\n","    else:\n","        np_image = cv2.imread(path)  # may return a string, in which case treat it as a path\n","    \n","    image = np_image\n","    # image = cv2.cvtColor(np_image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, (image_dim, image_dim))\n","    # image = resize_image(image, image_dim)\n","    \n","    # if median_blur:\n","    #     k = np.max(image.shape)//20*2+1\n","    #     blur = cv2.medianBlur(image, k)\n","    # else:\n","    #     blur = cv2.GaussianBlur( image , (0,0) , sigmaX) \n","        \n","    # image = cv2.addWeighted ( image,4, blur,-4 ,128)\n","\n","    # image = Radius_Reduction(image, PARAM)\n","\n","    return pil2tensor(image, np.float32).div_(255) # return tensor\n","\n","def advprop_normalise(torch_tensor):\n","    np_image = image2np(torch_tensor) * 2.0 - 1.0 \n","    return pil2tensor(np_image, np.float32) # return tensor\n","\n","# we later override the load image function of fast.ai to make sure all images\n","# including train and test sets, are properly processed\n","# define function    \n","\n","# def _load_format(path, convert_mode, after_open, image_dim)->Image:\n","#     image = contrast_and_crop(None, image_dim, path, median_blur=False) \n","# \n","#     return Image(image) # return fastai Image format\n","\n","print(\"initialised preprocessing\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["initialised preprocessing\n"],"name":"stdout"}]}]}